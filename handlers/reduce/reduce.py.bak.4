import os
import os.path
import logging
import glob
import multiprocessing
import xarray as xr
import numpy as np
from instradatore.router import MyException

def riduci_dati(prefix, current, nextt, root_lavoro, out_type, variabili_selezionate, z_type):
    """
    Reduce the data of a NetCDF file.

    Args:
        prefix (str): File prefix.
        current (str): Current period.
        nextt (str): Next period.
        root_lavoro (str): Root path of working files.
        out_type (str): Output type.
        variabili_selezionate (list): Selected variables to keep in the reduced dataset.
        z_type (str): Type of vertical coordinate used.

    Returns:
        None
    """
    url = f"{root_lavoro}/{prefix}.atmos_{out_type}_Ls{current}_{nextt}_{z_type}.nc"
    logging.debug(f"Apro {url}")
    
    try:
        ds = xr.open_dataset(url, decode_times=False)

        logging.debug(f"Dataset dimensions: {ds.dims}")
        logging.debug(f"Available variables: {list(ds.variables.keys())}")
    except Exception as e:
        logging.debug(f"**** An exception occurred:{type(e).__name__} {url=}")
        raise MyException(f"{e}")

    
    OV_VAL = 9.96921e+36

    def first_valid_value(array):
        """
        Retrieve the first valid value in an array, ignoring NaNs.

        Args:
            array (np.ndarray): Input array.

        Returns:
            float: First valid value or NaN if none are valid.
        """
        valid_mask = ~np.isnan(array)
        if np.any(valid_mask):
            return array[valid_mask][0]
        else:
            return np.nan

    if 'temp' in ds:
        try:
            logging.debug(f"Calculating surf_temp... in {url}")
            ds['temp'] = xr.where(ds['temp'] == OV_VAL, np.nan, ds['temp'])
            ds['surf_temp'] = xr.apply_ufunc(
                first_valid_value,
                ds['temp'],
                input_core_dims=[[z_type]],
                vectorize=True,
                dask="allowed",
                output_dtypes=[float]
            )
            ds['surf_temp'].load()  # Forza il calcolo e cattura eventuali errori
            logging.debug(f"terminato surf_temp in {url}")
        except Exception as e:
            logging.debug(f"** temp ** An exception occurred:{type(e).__name__}")
            raise MyException(f"{e}")

    if 'rho' in ds:
        try:
            logging.debug(f"Calculating rho_f... in {url}")
            ds['rho'] = xr.where(ds['rho'] == OV_VAL, np.nan, ds['rho'])
            ds['rho_f'] = xr.apply_ufunc(
                first_valid_value,
                ds['rho'],
                input_core_dims=[[z_type]],
                vectorize=True,
                dask="allowed",
                output_dtypes=[float]
            )
            ds['rho_f'].load()  # Forza il calcolo e cattura eventuali errori
            logging.debug(f"terminato rho_f in {url}")
        except Exception as e:
            logging.debug(f"** rho ** An exception occurred:{type(e).__name__}")
            raise MyException(f"{e}")

    if 'd_dz_ucomp' in ds:
        try:
            logging.debug(f"Calculating d_dz_ucomp at the surface... in {url}")
            ds['d_dz_ucomp'] = xr.where(ds['d_dz_ucomp'] == OV_VAL, np.nan, ds['d_dz_ucomp'])
            ds['dz_surf_ucomp'] = xr.apply_ufunc(
                first_valid_value,
                ds['d_dz_ucomp'],
                input_core_dims=[[z_type]],
                vectorize=True,
                dask="allowed",
                output_dtypes=[float]
            )
            ds['dz_surf_ucomp'].load()  # Forza il calcolo e cattura eventuali errori
            logging.debug(f"terminato d_dz_ucomp in {url}")
        except Exception as e:
            logging.debug(f"** d_dz_ucomp ** An exception occurred:{type(e).__name__}")
            raise MyException(f"{e}")


    if 'd_dz_vcomp' in ds:
        try:
            logging.debug(f"Calculating d_dz_vcomp at the surface... in {url}")
            ds['d_dz_vcomp'] = xr.where(ds['d_dz_vcomp'] == OV_VAL, np.nan, ds['d_dz_vcomp'])
            ds['dz_surf_vcomp'] = xr.apply_ufunc(
                first_valid_value,
                ds['d_dz_vcomp'],
                input_core_dims=[[z_type]],
                vectorize=True,
                dask="allowed",
                output_dtypes=[float]
            )
            ds['dz_surf_vcomp'].load()  # Forza il calcolo e cattura eventuali errori
            logging.debug(f"terminato d_dz_vcomp in {url}")
        except Exception as e:
            logging.debug(f"** d_dz:vcomp ** An exception occurred:{type(e).__name__}")
            raise MyException(f"{e}")

    logging.debug(f"Seleziono variabili in {url}")
    new_ds = ds[variabili_selezionate]

    file_dest = f"{root_lavoro}/{prefix}.atmos_{out_type}_Ls{current}_{nextt}_{z_type}_sel.nc"

    logging.debug(f"Salvo {file_dest}")
    new_ds.to_netcdf(file_dest)
    logging.debug(f"Salvato {file_dest}")

class Reduce():
    def __init__(self, report, **env):
        """
        Split atmos_daily simulation datafile to fit memory constraints.

        Args:
            report (object): Report object to log the output.
            **env (dict): Environment variables containing simulation parameters.

        Raises:
            MyException: If any required environment variable is missing or invalid.
        """
        self.env = env
        logging.debug(f"class Riduci(), {self.env=}")

        report_header = self.__class__.__init__
        report_msg = ""
        err = 0

        if os.cpu_count() <= int(self.env['max_threads'] + 1):
            self.env['max_threads'] = os.cpu_count() - 1
        logging.debug(f"Instanzio Riduci con {self.env['max_threads']=}")

        self.report = report

        required_vars = ['root_lavoro', 'periodi', 'out_type', 'variabili_selezionate', 'z_type']
        for var in required_vars:
            if var not in self.env or not self.env[var]:
                raise MyException(f"Missing or invalid environment variable: {var}")

        root_lavoro = self.env['root_lavoro']
        periodi = self.env['periodi']
        max_threads = int(self.env['max_threads'])
        out_type = self.env['out_type']
        variabili_selezionate = self.env['variabili_selezionate']
        z_type = self.env['z_type']

        pool = multiprocessing.Pool(processes=max_threads)
        tasks = []

        try:
            for i in range(len(periodi) - 1):
                current = periodi[i]
                _next = periodi[i + 1]

                files = [f for f in os.listdir(root_lavoro) if f.endswith(f"atmos_{out_type}_Ls{current}_{_next}.nc")]
                for file in files:
                    prefix = file.split('.')[0]

                logging.debug(f"Aggiungo task per {prefix=}, {current=}, {_next=}")

                tasks.append(pool.apply_async(riduci_dati, args=(prefix, current, _next, root_lavoro, out_type, variabili_selezionate, z_type)))

            pool.close()

            for task in tasks:
                task.get()  # Attende la fine di ogni task e raccoglie eccezioni

        except Exception as e:
            logging.debug(f"An exception occurred:{type(e).__name__} {prefix=} {current=}") 
            raise MyException(f"{e}")
        finally:
            pool.join()

