import os
import os.path
import logging
import subprocess


from datetime import datetime, time
import pytz
import pandas as pd
import numpy as np
import xarray as xr
import cftime
from pydap.client import open_url
from pydap.cas.urs import setup_session
import sys

#prefix = sys.argv[1]  
#current = sys.argv[2]  
#nextt = sys.argv[3]
#root_lavoro = sys.argv[4]

url = f"{root_lavoro}/{prefix}.atmos_daily_Ls{current}_{nextt}_zstd.nc"
print(f"Apro {url}")
ds = xr.open_dataset(url, decode_times=False)

###
# Function to select the first non-NaN value along the 'zstd' dimension
###
OV_VAL = 9.96921e+36  # CAP post-processig induces the overflow on this numeric value
def first_valid_value(array):
    valid_mask = ~np.isnan(array)
    if np.any(valid_mask):
        return array[valid_mask][0]
    else:
        return np.nan
###
# Atmospheric temp at the surface
###
print('Calculating surf_temp...')
ds['temp'] = xr.where(ds['temp'] == OV_VAL, np.nan, ds['temp'])
ds['surf_temp'] = xr.apply_ufunc(
    first_valid_value,
    ds['temp'],
    input_core_dims=[['zstd']],
    vectorize=True,
    dask="parallelized",
    output_dtypes=[float]
)

###
# Atmospheric density at the surface 'ρf'
###
print('Calculating rho_f...')
ds['rho'] = xr.where(ds['rho'] == OV_VAL, np.nan, ds['rho'])
ds['rho_f'] = xr.apply_ufunc(
    first_valid_value,
    ds['rho'],
    input_core_dims=[['zstd']],
    vectorize=True,
    dask="parallelized",
    output_dtypes=[float]
)

###
# d_dz_u(and v)comp  at the surface
###
print('Calculating d_dz_ucomp at the surface...')
ds['d_dz_ucomp'] = xr.where(ds['d_dz_ucomp'] == OV_VAL, np.nan, ds['d_dz_ucomp'])
ds['dz_surf_ucomp'] = xr.apply_ufunc(
    first_valid_value,
    ds['d_dz_ucomp'],
    input_core_dims=[['zstd']],
    vectorize=True,
    dask="parallelized",
    output_dtypes=[float]
)

print('Calculating d_dz_vcomp at the surface...')
ds['d_dz_vcomp'] = xr.where(ds['d_dz_vcomp'] == OV_VAL, np.nan, ds['d_dz_vcomp'])
ds['dz_surf_vcomp'] = xr.apply_ufunc(
    first_valid_value,
    ds['d_dz_vcomp'],
    input_core_dims=[['zstd']],
    vectorize=True,
    dask="parallelized",
    output_dtypes=[float]
)


variabili_selezionate = ds[['rho', 'rho_f', 'stress', 'ukd', 'vkd', 'ps', 'surf_temp', 'co2ice_sfc', 'dz_surf_ucomp', 'dz_surf_vcomp']]

new_ds = variabili_selezionate

file_dest = f"{root_lavoro}/{prefix}.atmos_daily_Ls{current}_{nextt}_zstd_sel.nc"

print(f"Salvo {file_dest}")

new_ds.to_netcdf(file_dest)




class Riduci():
    def __init__(self, report, **env):
        """ Split atmos_daily simulation datafile to fit memory constraints.

        Args:
            max_threads (int, optional): _description_. Defaults to 5.
        """
        self.env = env
        logging.debug(f"class Riduci(), {self.env=}")

        # Determina il numero massimo di thread
        if os.cpu_count() <= int(self.env['max_threads'] + 1):
            self.env['max_threads'] = os.cpu_count() - 1
        logging.debug(f"Instanzio Altitude con {self.env['max_threads']=}")

        running_processes = []
        try:
            sol_file_dati = self.env['sol_file_dati']
            root_lavoro = self.env['root_lavoro']
            periodi = self.env['periodi']
            logging.debug(f"{periodi=}")
            logging.debug(f"{len(periodi)=}")

            # Lancia i processi per ogni periodo
            for i in range(len(periodi) - 1):  
                current = periodi[i]
                _next = periodi[i + 1]
                
                files = [f for f in os.listdir(root_lavoro) if f.endswith(f"atmos_daily_Ls{current}_{_next}.nc")]
                for file in files:
                    prefix = file.split('.')[0]

                logging.debug(f"Faccio riduzione con {prefix=} {current=} e {_next=}")
                
                
                '''
                # Lancia il processo in modalità asincrona
                process = subprocess.Popen(
                    ["bash", script_path, str(prefix), str(current), str(_next), str(root_lavoro)],
                    stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True
                )
                running_processes.append(process)
                '''

                # Se il numero di processi in esecuzione è uguale al numero massimo di thread, aspetta che terminino
                if len(running_processes) >= int(self.env['max_threads']):
                    logging.debug("Raggiunto max_threads, in attesa...")
                    # Attende che i processi correnti terminino
                    for p in running_processes:
                        stdout, stderr = p.communicate()  # Attende e cattura output
                        report.add(f"Process {p.pid}, \n\tstdout: {stdout} \n\tstderr: {stderr}")
                    running_processes.clear()  # Pulisce la lista dei processi in esecuzione

            # Attende il completamento dei processi rimanenti
            for p in running_processes:
                stdout, stderr = p.communicate()
                report.add(f"Process {p.pid}, \n\tstdout: {stdout} \n\tstderr: {stderr}")

        except subprocess.CalledProcessError as e:
            print("Errore nell'esecuzione dello script:")
            print(e.stderr)
            logging.debug(e.stderr)

